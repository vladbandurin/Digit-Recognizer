{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnLS7MnmED0y"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVKJJLLhEOLT"
   },
   "outputs": [],
   "source": [
    "n_classes = 8\n",
    "\n",
    "images = [cv2.imread(file) for class_number in range(1,n_classes+1) for file in glob.glob('data/'+ str(class_number) + '/*.jpg')]\n",
    "images = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMdfYlsBJgDw"
   },
   "outputs": [],
   "source": [
    "Y = [[class_number] for class_number in range(1,n_classes+1) for i in range(len(glob.glob('data/'+ str(class_number) + '/*.jpg')))]\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IQCQJ1teHhLk",
    "outputId": "ec484366-5338-47ec-951d-b9b619991a99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5600, 28, 28, 3), (5600, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "JCyT_wtWYPpF",
    "outputId": "971f6c91-804a-416a-ff38-2e288d8d1ae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (4480, 28, 28, 3) (4480, 1)\n",
      "Test:  (1120, 28, 28, 3) (1120, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4480, 8), (1120, 8))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, Y, test_size=0.2, \n",
    "                                                    random_state=42, stratify=Y)\n",
    "\n",
    "print('Train: ', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)\n",
    "\n",
    "y_train = pd.get_dummies(pd.DataFrame(y_train, columns=['label']), columns=['label']).values\n",
    "y_test = pd.get_dummies(pd.DataFrame(y_test, columns=['label']), columns=['label']).values\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWz0AhDdEZ3U"
   },
   "outputs": [],
   "source": [
    "# convert in float and normalize to scale (0, 1)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255.\n",
    "X_test /= 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSb96zvKFL_D"
   },
   "source": [
    "**LeNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZeZ4rQxVch3"
   },
   "outputs": [],
   "source": [
    "def create_weights(name, shape):\n",
    "    return tf.get_variable(name=name, shape=shape, initializer=tf.initializers.he_normal())\n",
    "  \n",
    "  \n",
    "  \n",
    "def create_biases(size):\n",
    "    return tf.Variable(tf.constant(0.1, shape=[size]))\n",
    "\n",
    "\n",
    "\n",
    "def create_convolutional_layer(input_tensor,\n",
    "                               num_input_channels, \n",
    "                               conv_filter_size,        \n",
    "                               num_filters,\n",
    "                               name): \n",
    "    \n",
    "    weights = create_weights(name, shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n",
    "\n",
    "    biases = create_biases(num_filters)\n",
    "\n",
    "    layer = tf.nn.conv2d(input=input_tensor,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')    \n",
    "\n",
    "    layer += biases\n",
    "\n",
    "    layer = tf.nn.max_pool(value=layer,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='SAME')\n",
    "  \n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "\n",
    "def create_flatten_layer(layer):\n",
    "    \n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer\n",
    "\n",
    " \n",
    "\n",
    "def create_fc_layer(input, \n",
    "                    num_inputs,    \n",
    "                    num_outputs,\n",
    "                    name,\n",
    "                    use_bn=True,\n",
    "                    use_relu=True):\n",
    "  \n",
    "    \n",
    "    weights = create_weights(name, shape=[num_inputs, num_outputs])\n",
    "    biases = create_biases(num_outputs)\n",
    "\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "  \n",
    "    if use_bn:\n",
    "        batch_mean, batch_var = tf.nn.moments(layer, [0])\n",
    "        beta = tf.Variable(tf.zeros([num_outputs]))\n",
    "        scale = tf.Variable(tf.ones([num_outputs]))\n",
    "        bn = tf.nn.batch_normalization(layer, batch_mean, batch_var, beta, scale, 0.001)\n",
    "  \n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(bn)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dR6jbXIGnT5s"
   },
   "outputs": [],
   "source": [
    "def train(batch_size, epochs, keep_proba):\n",
    "  \n",
    "    for j in range(epochs+1): \n",
    "           \n",
    "        for i in range(len(X_train) // batch_size + 1):\n",
    "      \n",
    "            if i == len(X_train) // batch_size and len(X_train) % batch_size != 0:\n",
    "                batch_xs, batch_ys = X_train[i*batch_size:], y_train[i*batch_size:]\n",
    "            elif i == len(X_train) // batch_size and len(X_train) % batch_size == 0:\n",
    "                pass\n",
    "            else:\n",
    "                batch_xs, batch_ys = X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "            sess.run(train_step, feed_dict={x_image: batch_xs, y: batch_ys, keep_probability: keep_proba})\n",
    "    \n",
    "    if j % 10 == 0:\n",
    "        print('Epoch = {epoch:>4}; Train Accuracy = {train_acc:>5.4}; Test Accuracy = {test_acc:>5.4}'.format(epoch=j,\n",
    "                                                                                                        train_acc=get_train_accuracy(), \n",
    "                                                                                                        test_acc=get_test_accuracy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSvASBUUDvR0"
   },
   "outputs": [],
   "source": [
    "def get_train_accuracy(batch_size=256):\n",
    "    \n",
    "    \n",
    "    list_of_acc = []\n",
    "  \n",
    "    for i in range(len(X_train) // batch_size + 1):\n",
    "        \n",
    "        if i == len(X_train) // batch_size and len(X_train) % batch_size != 0:\n",
    "            batch_xs, batch_ys = X_train[i*batch_size:], y_train[i*batch_size:]\n",
    "        elif i == len(X_train) // batch_size and len(X_train) % batch_size == 0:\n",
    "            pass\n",
    "        else:\n",
    "            batch_xs, batch_ys = X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "  \n",
    "        tmp_acc = sess.run(accuracy, feed_dict={x_image: batch_xs, y: batch_ys, keep_probability: 1.})\n",
    "        list_of_acc.append(tmp_acc)\n",
    "    \n",
    "    acc = np.mean(list_of_acc)\n",
    "  \n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def get_test_accuracy(batch_size=len(X_test)):\n",
    "    \n",
    "    \n",
    "    list_of_acc = []\n",
    "  \n",
    "    for i in range(len(X_test) // batch_size + 1):\n",
    "        if i == len(X_test) // batch_size and len(X_test) % batch_size != 0:\n",
    "            batch_xs, batch_ys = X_test[i*batch_size:], y_test[i*batch_size:]\n",
    "        elif i == len(X_test) // batch_size and len(X_test) % batch_size == 0:\n",
    "            pass\n",
    "        else:\n",
    "            batch_xs, batch_ys = X_test[i*batch_size:(i+1)*batch_size], y_test[i*batch_size:(i+1)*batch_size]\n",
    "  \n",
    "    tmp_acc = sess.run(accuracy, feed_dict={x_image: batch_xs, y: batch_ys, keep_probability: 1.})\n",
    "    list_of_acc.append(tmp_acc)\n",
    "    \n",
    "    acc = np.mean(list_of_acc)\n",
    "  \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "_NGbO1bzoKlJ",
    "outputId": "f8f7d798-bab5-49f2-9eb4-404437d8c66d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =    0; Train Accuracy = 0.2257; Test Accuracy = 0.2259\n",
      "Epoch =   10; Train Accuracy = 0.4805; Test Accuracy = 0.4804\n",
      "Epoch =   20; Train Accuracy = 0.5972; Test Accuracy = 0.5866\n",
      "Epoch =   30; Train Accuracy = 0.6968; Test Accuracy = 0.6929\n",
      "Epoch =   40; Train Accuracy = 0.7812; Test Accuracy = 0.7812\n",
      "Epoch =   50; Train Accuracy = 0.8175; Test Accuracy = 0.8232\n",
      "Epoch =   60; Train Accuracy = 0.8516; Test Accuracy = 0.8536\n",
      "Epoch =   70; Train Accuracy = 0.8859; Test Accuracy = 0.8857\n",
      "Epoch =   80; Train Accuracy = 0.9108; Test Accuracy = 0.9027\n",
      "Epoch =   90; Train Accuracy = 0.9323; Test Accuracy = 0.9223\n",
      "Epoch =  100; Train Accuracy = 0.9455; Test Accuracy = 0.9286\n",
      "Epoch =  110; Train Accuracy = 0.9564; Test Accuracy = 0.942\n",
      "Epoch =  120; Train Accuracy = 0.9622; Test Accuracy = 0.9446\n",
      "Epoch =  130; Train Accuracy = 0.9677; Test Accuracy =  0.95\n",
      "Epoch =  140; Train Accuracy = 0.9724; Test Accuracy = 0.9509\n",
      "Epoch =  150; Train Accuracy = 0.9744; Test Accuracy = 0.9598\n",
      "Epoch =  160; Train Accuracy = 0.9792; Test Accuracy = 0.9607\n",
      "Epoch =  170; Train Accuracy = 0.9835; Test Accuracy = 0.9625\n",
      "Epoch =  180; Train Accuracy = 0.9833; Test Accuracy = 0.9643\n",
      "Epoch =  190; Train Accuracy = 0.987; Test Accuracy = 0.9688\n",
      "Epoch =  200; Train Accuracy = 0.9883; Test Accuracy = 0.9688\n",
      "Epoch =  210; Train Accuracy = 0.9907; Test Accuracy = 0.9705\n",
      "Epoch =  220; Train Accuracy = 0.9915; Test Accuracy = 0.9723\n",
      "Epoch =  230; Train Accuracy = 0.9937; Test Accuracy = 0.9741\n",
      "Epoch =  240; Train Accuracy = 0.9928; Test Accuracy = 0.9732\n",
      "Epoch =  250; Train Accuracy = 0.9952; Test Accuracy = 0.9759\n",
      "Epoch =  260; Train Accuracy = 0.9959; Test Accuracy = 0.9759\n",
      "Epoch =  270; Train Accuracy = 0.9959; Test Accuracy = 0.9786\n",
      "Epoch =  280; Train Accuracy = 0.9985; Test Accuracy = 0.9759\n",
      "Epoch =  290; Train Accuracy = 0.9974; Test Accuracy = 0.9759\n",
      "Epoch =  300; Train Accuracy = 0.9987; Test Accuracy = 0.9759\n",
      "Epoch =  310; Train Accuracy = 0.9983; Test Accuracy = 0.9777\n",
      "Epoch =  320; Train Accuracy = 0.9991; Test Accuracy = 0.9777\n",
      "Epoch =  330; Train Accuracy = 0.9993; Test Accuracy = 0.9768\n",
      "Epoch =  340; Train Accuracy = 0.9993; Test Accuracy = 0.975\n",
      "Epoch =  350; Train Accuracy = 0.9991; Test Accuracy = 0.9804\n",
      "Epoch =  360; Train Accuracy = 0.9996; Test Accuracy = 0.9804\n",
      "Epoch =  370; Train Accuracy = 0.9998; Test Accuracy = 0.9777\n",
      "Epoch =  380; Train Accuracy = 0.9996; Test Accuracy = 0.9768\n",
      "Epoch =  390; Train Accuracy = 0.9998; Test Accuracy = 0.9812\n",
      "Epoch =  400; Train Accuracy = 0.9998; Test Accuracy = 0.9812\n",
      "Epoch =  410; Train Accuracy =   1.0; Test Accuracy = 0.9821\n",
      "Epoch =  420; Train Accuracy =   1.0; Test Accuracy = 0.9812\n",
      "Epoch =  430; Train Accuracy =   1.0; Test Accuracy = 0.9812\n",
      "Epoch =  440; Train Accuracy =   1.0; Test Accuracy = 0.9795\n",
      "Epoch =  450; Train Accuracy =   1.0; Test Accuracy = 0.9804\n",
      "Epoch =  460; Train Accuracy =   1.0; Test Accuracy = 0.9795\n",
      "Epoch =  470; Train Accuracy =   1.0; Test Accuracy = 0.9804\n",
      "Epoch =  480; Train Accuracy =   1.0; Test Accuracy = 0.9795\n",
      "Epoch =  490; Train Accuracy = 0.9998; Test Accuracy = 0.9777\n",
      "Epoch =  500; Train Accuracy =   1.0; Test Accuracy = 0.9812\n",
      "CPU times: user 1min 7s, sys: 38.7 s, total: 1min 46s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders\n",
    "x_image = tf.placeholder(tf.float32, [None, 28, 28, 3], name='x')\n",
    "y = tf.placeholder(tf.float32, [None, 8], name='y')\n",
    "keep_probability = tf.placeholder(tf.float32, name='keep_proba')\n",
    "\n",
    "# layers of NN\n",
    "layer_conv_1 = create_convolutional_layer(x_image, 3, 5, 32, 'w1')\n",
    "\n",
    "layer_conv_2 = create_convolutional_layer(layer_conv_1, 32, 5, 64, 'w2')\n",
    "\n",
    "h_avg_pool = tf.nn.avg_pool(layer_conv_2, ksize=[1, 7, 7, 1], strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "\n",
    "layer_flat = create_flatten_layer(h_avg_pool)\n",
    "\n",
    "layer_fc1 = create_fc_layer(layer_flat, layer_flat.get_shape()[1:4].num_elements(), \n",
    "                            256, 'w3')\n",
    "\n",
    "layer_fc1_dpor = tf.nn.dropout(layer_fc1, keep_probability)\n",
    "\n",
    "logit_conv = create_fc_layer(layer_fc1_dpor, 256, 8, 'w4', False, False)\n",
    "\n",
    "y_conv = tf.nn.softmax(logit_conv)\n",
    "\n",
    "# define a error and add optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit_conv, labels=y))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "# estimate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accur')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "train(batch_size=len(X_train), epochs=500, keep_proba=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-Cb7kInDsk7G",
    "outputId": "219164c9-4028-4758-8df5-f2b203117f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 1.0\n",
      "Test Accuracy = 0.9910714030265808\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy = {}'.format(get_train_accuracy()))\n",
    "\n",
    "print('Test Accuracy = {}'.format(get_test_accuracy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckc8ZQeuHjpA"
   },
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/model/Classifier'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'model/Classifier')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Modeling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
